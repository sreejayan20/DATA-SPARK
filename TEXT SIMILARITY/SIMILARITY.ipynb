{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90629922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\anaconda\\lib\\site-packages (3.4.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\anaconda\\lib\\site-packages (from spacy) (8.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda\\lib\\site-packages (from spacy) (61.2.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\anaconda\\lib\\site-packages (from spacy) (1.22.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\anaconda\\lib\\site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\anaconda\\lib\\site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\anaconda\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\anaconda\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\anaconda\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\anaconda\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\anaconda\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\anaconda\\lib\\site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\anaconda\\lib\\site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\anaconda\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\anaconda\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\anaconda\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in c:\\anaconda\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\anaconda\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\anaconda\\lib\\site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\anaconda\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\anaconda\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\anaconda\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\anaconda\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\anaconda\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\anaconda\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c6983ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading omw-1.4: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nlp = spacy.load('en_core_web_sm')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d954316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.pipeline import Pipeline\n",
    "import spacy\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "82ff9888",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('Precily.csv')\n",
    "data.head(1)\n",
    "cpy=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a100e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "38827daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                                                   text1  \\\n",
       "0     broadband challenges tv viewing the number of ...   \n",
       "1     rap boss arrested over drug find rap mogul mar...   \n",
       "2     player burn-out worries robinson england coach...   \n",
       "3     hearts of oak 3-2 cotonsport hearts of oak set...   \n",
       "4     sir paul rocks super bowl crowds sir paul mcca...   \n",
       "...                                                 ...   \n",
       "2995  uk directors guild nominees named martin scors...   \n",
       "2996  u2 to play at grammy awards show irish rock ba...   \n",
       "2997  pountney handed ban and fine northampton coach...   \n",
       "2998  belle named  best scottish band  belle & sebas...   \n",
       "2999  criminal probe on citigroup deals traders at u...   \n",
       "\n",
       "                                                  text2  \n",
       "0     gardener wins double in glasgow britain s jaso...  \n",
       "1     amnesty chief laments war failure the lack of ...  \n",
       "2     hanks greeted at wintry premiere hollywood sta...  \n",
       "3     redford s vision of sundance despite sporting ...  \n",
       "4     mauresmo opens with victory in la amelie maure...  \n",
       "...                                                 ...  \n",
       "2995  steel firm  to cut  45 000 jobs mittal steel  ...  \n",
       "2996  israel looks to us for bank chief israel has a...  \n",
       "2997  india and iran in gas export deal india has si...  \n",
       "2998  mido makes third apology ahmed  mido  hossam h...  \n",
       "2999  former ni minister scott dies former northern ...  \n",
       "\n",
       "[3000 rows x 2 columns]>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpy.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e0cee521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpy.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4b5b671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp=cpy.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1270d0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(cpy):\n",
    "  pattern = r'[' + string.punctuation + ']'\n",
    "  cpy['text1']=data['text1'].map(lambda m:re.sub(pattern,\" \",m))\n",
    "  cpy['text2']=data['text2'].map(lambda m:re.sub(pattern,\" \",m))\n",
    "  return cpy\n",
    "\n",
    "\n",
    "def lower(cpy):\n",
    "  cpy['text1']=cpy['text1'].map(lambda m:m.lower())\n",
    "  cpy['text2']=cpy['text2'].map(lambda m:m.lower())\n",
    "  return cpy\n",
    "\n",
    "\n",
    "def tokenization(text):\n",
    "    tokens = re.split(' ',text)\n",
    "    return tokens\n",
    "\n",
    "def token(cpy):\n",
    "  cpy['text1']= cpy['text1'].apply(lambda x: tokenization(x))\n",
    "  cpy['text2']= cpy['text2'].apply(lambda x: tokenization(x))\n",
    "  return cpy\n",
    "\n",
    "\n",
    "sw=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_SW(cpy):\n",
    "   cpy['text1']=cpy['text1'].apply(lambda x: [item for item in x if item not in sw])\n",
    "   cpy['text2']=cpy['text2'].apply(lambda x: [item for item in x if item not in sw])\n",
    "   return cpy\n",
    "\n",
    "\n",
    "def remove_digits(cpy):\n",
    "  cpy['text1']=cpy['text1'].apply(lambda x: [item for item in x if not item.isdigit()])\n",
    "  cpy['text2']=cpy['text2'].apply(lambda x: [item for item in x if not item.isdigit()])\n",
    "  return cpy\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(cpy):\n",
    "  cpy['text1']=cpy['text1'].apply(lambda x: [lemmatizer.lemmatize(item) for item in x])\n",
    "  cpy['text2']=cpy['text2'].apply(lambda x: [lemmatizer.lemmatize(item) for item in x])\n",
    "  return cpy\n",
    "\n",
    "\n",
    "def remove_empty_tokens(cpy):\n",
    "  cpy['text1']=cpy['text1'].apply(lambda x: [item for item in x if item !=''])\n",
    "  cpy['text2']=cpy['text2'].apply(lambda x: [item for item in x if item !=''])\n",
    "  return cpy\n",
    "\n",
    "\n",
    "def remove_single_letters(cpy):\n",
    "  cpy['text1']=cpy['text1'].apply(lambda x: [item for item in x if len(item) > 1])\n",
    "  cpy['text2']=cpy['text2'].apply(lambda x: [item for item in x if len(item) > 1])\n",
    "  return cpy\n",
    "\n",
    "\n",
    "def detoken(cpy):\n",
    "  cpy['text1']= cpy['text1'].apply(lambda x: TreebankWordDetokenizer().detokenize(x))\n",
    "  cpy['text2']= cpy['text2'].apply(lambda x: TreebankWordDetokenizer().detokenize(x))\n",
    "  return cpy\n",
    "\n",
    "def replace_spaces(x,space,second):\n",
    "  result = x.replace(space, second)\n",
    "  return result\n",
    "def remove_space(cpy):\n",
    "  cpy['text1']= cpy['text1'].apply(lambda x: replace_spaces(x,'  ',' '))\n",
    "  cpy['text2']= cpy['text2'].apply(lambda x: replace_spaces(x,'  ',' '))\n",
    "  return cpy\n",
    "def count_vcr():\n",
    "  for i in range(len(cpy)):\n",
    "    doc1=cpy['text1'][i]\n",
    "    doc2=cpy['text2'][i]\n",
    "    docs=(doc1,doc2)\n",
    "    matrix = CountVectorizer().fit_transform(docs)\n",
    "    cosine_sim = cosine_similarity(matrix[0], matrix[1])\n",
    "    similarity.append(cosine_sim)\n",
    "  return similarity\n",
    "\n",
    "def similarity_fn():\n",
    "  for i in range(len(cpy)):\n",
    "    doc1=cpy['text1'][i]\n",
    "    doc2=cpy['text2'][i]\n",
    "    docs=(doc1,doc2)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(docs)\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
    "    similarity.append(cosine_sim)\n",
    "  return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f09066b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpyd=cpy.pipe(remove_punc).pipe(token).pipe(remove_SW).pipe(remove_digits).pipe(lemmatize).pipe(remove_empty_tokens).pipe(remove_single_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "637dd1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>broadband challenge tv viewing number european...</td>\n",
       "      <td>gardener win double glasgow britain jason gard...</td>\n",
       "      <td>[[0.11225730287187921]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rap bos arrested drug find rap mogul marion su...</td>\n",
       "      <td>amnesty chief lament war failure lack public o...</td>\n",
       "      <td>[[0.03676246991128057]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>player burn worry robinson england coach andy ...</td>\n",
       "      <td>hank greeted wintry premiere hollywood star to...</td>\n",
       "      <td>[[0.06682109510306634]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heart oak cotonsport heart oak set ghanaian co...</td>\n",
       "      <td>redford vision sundance despite sporting cordu...</td>\n",
       "      <td>[[0.03993800591673614]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sir paul rock super bowl crowd sir paul mccart...</td>\n",
       "      <td>mauresmo open victory la amelie mauresmo maria...</td>\n",
       "      <td>[[0.09826473199208315]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text1  \\\n",
       "0  broadband challenge tv viewing number european...   \n",
       "1  rap bos arrested drug find rap mogul marion su...   \n",
       "2  player burn worry robinson england coach andy ...   \n",
       "3  heart oak cotonsport heart oak set ghanaian co...   \n",
       "4  sir paul rock super bowl crowd sir paul mccart...   \n",
       "\n",
       "                                               text2               Similarity  \n",
       "0  gardener win double glasgow britain jason gard...  [[0.11225730287187921]]  \n",
       "1  amnesty chief lament war failure lack public o...  [[0.03676246991128057]]  \n",
       "2  hank greeted wintry premiere hollywood star to...  [[0.06682109510306634]]  \n",
       "3  redford vision sundance despite sporting cordu...  [[0.03993800591673614]]  \n",
       "4  mauresmo open victory la amelie mauresmo maria...  [[0.09826473199208315]]  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_converter = CountVectorizer()\n",
    "cpyd.pipe(detoken).pipe(remove_space)\n",
    "similarity=[]\n",
    "similarity=count_vcr()\n",
    "data_cvr=cpyd.copy()\n",
    "data_cvr['Similarity']=similarity\n",
    "data_cvr[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f87fc2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "similarity=[]\n",
    "similarity=similarity_fn()\n",
    "data_tf=cpyd.copy()\n",
    "data_tf['Similarity']=similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d4c5fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=data_cvr.copy()\n",
    "all_data['Count-Vec Similarity']=all_data['Similarity']\n",
    "all_data=all_data.drop('Similarity',axis=1)\n",
    "all_data['Tf-idf Similarity']=data_tf['Similarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7bdcaed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>Count-Vec Similarity</th>\n",
       "      <th>Tf-idf Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>broadband challenge tv viewing number european...</td>\n",
       "      <td>gardener win double glasgow britain jason gard...</td>\n",
       "      <td>[[0.11225730287187921]]</td>\n",
       "      <td>[[0.0644185576608853]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rap bos arrested drug find rap mogul marion su...</td>\n",
       "      <td>amnesty chief lament war failure lack public o...</td>\n",
       "      <td>[[0.03676246991128057]]</td>\n",
       "      <td>[[0.019416324014600878]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>player burn worry robinson england coach andy ...</td>\n",
       "      <td>hank greeted wintry premiere hollywood star to...</td>\n",
       "      <td>[[0.06682109510306634]]</td>\n",
       "      <td>[[0.0357029291591446]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heart oak cotonsport heart oak set ghanaian co...</td>\n",
       "      <td>redford vision sundance despite sporting cordu...</td>\n",
       "      <td>[[0.03993800591673614]]</td>\n",
       "      <td>[[0.020703321019893143]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sir paul rock super bowl crowd sir paul mccart...</td>\n",
       "      <td>mauresmo open victory la amelie mauresmo maria...</td>\n",
       "      <td>[[0.09826473199208315]]</td>\n",
       "      <td>[[0.05451264233660033]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>uk director guild nominee named martin scorses...</td>\n",
       "      <td>steel firm cut job mittal steel one world larg...</td>\n",
       "      <td>[[0.015269597956778631]]</td>\n",
       "      <td>[[0.0077935579761156774]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>u2 play grammy award show irish rock band u2 p...</td>\n",
       "      <td>israel look bank chief israel asked banker for...</td>\n",
       "      <td>[[0.045609399309187955]]</td>\n",
       "      <td>[[0.023669183711536344]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>pountney handed ban fine northampton coach bud...</td>\n",
       "      <td>india iran gas export deal india signed 40bn £...</td>\n",
       "      <td>[[0.01354336246895791]]</td>\n",
       "      <td>[[0.006927652542377579]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>belle named best scottish band belle sebastian...</td>\n",
       "      <td>mido make third apology ahmed mido hossam made...</td>\n",
       "      <td>[[0.07393559564382367]]</td>\n",
       "      <td>[[0.039054361928280774]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>criminal probe citigroup deal trader banking g...</td>\n",
       "      <td>former ni minister scott dy former northern ir...</td>\n",
       "      <td>[[0.06407108738825835]]</td>\n",
       "      <td>[[0.034064353089584685]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text1  \\\n",
       "0     broadband challenge tv viewing number european...   \n",
       "1     rap bos arrested drug find rap mogul marion su...   \n",
       "2     player burn worry robinson england coach andy ...   \n",
       "3     heart oak cotonsport heart oak set ghanaian co...   \n",
       "4     sir paul rock super bowl crowd sir paul mccart...   \n",
       "...                                                 ...   \n",
       "2995  uk director guild nominee named martin scorses...   \n",
       "2996  u2 play grammy award show irish rock band u2 p...   \n",
       "2997  pountney handed ban fine northampton coach bud...   \n",
       "2998  belle named best scottish band belle sebastian...   \n",
       "2999  criminal probe citigroup deal trader banking g...   \n",
       "\n",
       "                                                  text2  \\\n",
       "0     gardener win double glasgow britain jason gard...   \n",
       "1     amnesty chief lament war failure lack public o...   \n",
       "2     hank greeted wintry premiere hollywood star to...   \n",
       "3     redford vision sundance despite sporting cordu...   \n",
       "4     mauresmo open victory la amelie mauresmo maria...   \n",
       "...                                                 ...   \n",
       "2995  steel firm cut job mittal steel one world larg...   \n",
       "2996  israel look bank chief israel asked banker for...   \n",
       "2997  india iran gas export deal india signed 40bn £...   \n",
       "2998  mido make third apology ahmed mido hossam made...   \n",
       "2999  former ni minister scott dy former northern ir...   \n",
       "\n",
       "          Count-Vec Similarity          Tf-idf Similarity  \n",
       "0      [[0.11225730287187921]]     [[0.0644185576608853]]  \n",
       "1      [[0.03676246991128057]]   [[0.019416324014600878]]  \n",
       "2      [[0.06682109510306634]]     [[0.0357029291591446]]  \n",
       "3      [[0.03993800591673614]]   [[0.020703321019893143]]  \n",
       "4      [[0.09826473199208315]]    [[0.05451264233660033]]  \n",
       "...                        ...                        ...  \n",
       "2995  [[0.015269597956778631]]  [[0.0077935579761156774]]  \n",
       "2996  [[0.045609399309187955]]   [[0.023669183711536344]]  \n",
       "2997   [[0.01354336246895791]]   [[0.006927652542377579]]  \n",
       "2998   [[0.07393559564382367]]   [[0.039054361928280774]]  \n",
       "2999   [[0.06407108738825835]]   [[0.034064353089584685]]  \n",
       "\n",
       "[3000 rows x 4 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba67216a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ef1b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91926733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f65857f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be0d507",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
